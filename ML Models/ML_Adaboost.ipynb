{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/teams.csv', delimiter=\";\", index_col=False)\n",
    "positions = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/positions.csv', delimiter=\";\", index_col=False)\n",
    "seasons = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/years.csv', delimiter=\";\", index_col=False)\n",
    "players = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/players.csv', delimiter=\";\", index_col=False)\n",
    "match_data = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/match_data.csv', delimiter=\";\", index_col=False)\n",
    "player_match_data = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/player_match_data_withid.csv', delimiter=\";\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge main two tables\n",
    "merged_data = player_match_data.merge(match_data, left_on='match_id', right_on='match_id', how='outer')\n",
    "merged_data = merged_data.drop('season_id_y', axis=1)\n",
    "merged_data = merged_data.drop('value', axis=1)\n",
    "merged_data = merged_data.drop('own_goals', axis=1) #Variation isn't large enough 104\n",
    "merged_data = merged_data.drop('red_cards', axis=1) #Variation isn't large enough 111\n",
    "merged_data = merged_data.drop('npxG', axis=1) #not using npg\n",
    "merged_data = merged_data.drop('h_nsxg', axis=1) #not using nsxg\n",
    "merged_data = merged_data.drop('a_nsxg', axis=1) #not using nsxg\n",
    "merged_data = merged_data.drop('player_match_ID', axis=1)\n",
    "merged_data = merged_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['p_team_spi'] = merged_data.apply(lambda row: row.h_team_spi if row.was_home == 1 else row.a_team_spi, axis=1)\n",
    "merged_data['oppn_spi'] = merged_data.apply(lambda row: row.a_team_spi if row.was_home == 1 else row.h_team_spi, axis=1)\n",
    "merged_data['prob_p_team_win'] = merged_data.apply(lambda row: row.prob_h_win if row.was_home == 1 else row.prob_a_win, axis=1)\n",
    "merged_data['prob_oppn_win'] = merged_data.apply(lambda row: row.prob_a_win if row.was_home == 1 else row.prob_h_win, axis=1)\n",
    "merged_data['p_team_proj_score'] = merged_data.apply(lambda row: row.h_proj_score if row.was_home == 1 else row.a_proj_score, axis=1)\n",
    "merged_data['oppn_team_proj_score'] = merged_data.apply(lambda row: row.a_proj_score if row.was_home == 1 else row.h_proj_score, axis=1)\n",
    "merged_data['importance_p_team'] = merged_data.apply(lambda row: row.importance_h if row.was_home == 1 else row.importance_a, axis=1)\n",
    "merged_data['importance_oppn_team'] = merged_data.apply(lambda row: row.importance_a if row.was_home == 1 else row.importance_h, axis=1)\n",
    "merged_data['score_p_team'] = merged_data.apply(lambda row: row.h_score if row.was_home == 1 else row.a_score, axis=1)\n",
    "merged_data['score_oppn_team'] = merged_data.apply(lambda row: row.a_score if row.was_home == 1 else row.h_score, axis=1)\n",
    "merged_data['xg_p_team'] = merged_data.apply(lambda row: row.h_xg if row.was_home == 1 else row.a_xg, axis=1)\n",
    "merged_data['xg_oppn_team'] = merged_data.apply(lambda row: row.a_xg if row.was_home == 1 else row.h_xg, axis=1)\n",
    "merged_data['opp_adv_spi'] = merged_data['oppn_spi'] - merged_data['p_team_spi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lagged varibaled and lagged moving average varibales\n",
    "#Sort values by player_id and date\n",
    "lagged_data = merged_data.sort_values(['player_id', 'date'])\n",
    "\n",
    "#Total points\n",
    "def average_form(var):\n",
    "    l1 = lagged_data.groupby('player_id')[var].shift(1) #Lagged once\n",
    "    l2 = lagged_data.groupby('player_id')[var].shift(2) #Lagged twice\n",
    "    l3 = lagged_data.groupby('player_id')[var].shift(3) #etc.\n",
    "    l4 = lagged_data.groupby('player_id')[var].shift(4)\n",
    "    lagged_data[var+'_lag_avg4'] = (l1+l2+l3+l4)/4 #etc.\n",
    "\n",
    "for i, var in enumerate(['total_points','xP','bonus','bps','minutes','goals','shots','xG','xA','assists','key_passes','npg','xGChain','xGBuildup','yellow_cards','clean_sheets','goals_conceded','penalties_missed','penalties_saved','saves','influence','creativity','threat','ict_index']):\n",
    "    average_form(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_data['return'] = lagged_data['total_points'].apply(lambda x: 1 if x > 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_data = lagged_data.drop(['date', 'prob_p_team_win', 'prob_oppn_win', 'total_points', 'xP', 'bonus', 'bps', 'goals', 'shots', 'xG', 'xA',\n",
    "       'assists', 'key_passes', 'npg', 'xGChain', 'xGBuildup', 'yellow_cards', 'clean_sheets', 'goals_conceded',\n",
    "       'penalties_missed', 'penalties_saved', 'saves', 'influence',\n",
    "       'creativity', 'threat', 'ict_index', 'h_team_spi', 'a_team_spi', 'prob_h_win', 'prob_a_win',\n",
    "       'h_proj_score', 'a_proj_score', 'importance_h', 'importance_a',\n",
    "       'h_score', 'a_score', 'h_xg', 'a_xg', 'score_p_team', 'score_oppn_team', 'xg_p_team', 'xg_oppn_team', \n",
    "       'match_id', 'player_id', 'player_team_id', 'season_id_x', 'round', 'h_team_id', 'a_team_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lagged_data = lagged_data[lagged_data['position_id'] == 1]\n",
    "#lagged_data = lagged_data[lagged_data['position_id'] == 1]\n",
    "lagged_data = lagged_data.dropna()\n",
    "lagged_data = lagged_data.drop(['minutes'], axis=1)\n",
    "#lagged_data = lagged_data.drop(column_importance_zero, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_STATE = 42 # for reproducible shuffling\n",
    "TT_RATIO = 0.20 # test/train\n",
    "y = lagged_data['return']\n",
    "X = lagged_data.drop(['return'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TT_RATIO, random_state=RAND_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4063\n",
       "1    4063\n",
       "Name: return, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def down_samp_rand(X, y, ratio=1):\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        rus = RandomUnderSampler(sampling_strategy=ratio, random_state=RAND_STATE)\n",
    "        X_rus, y_rus = rus.fit_resample(X, y)\n",
    "        return X_rus, y_rus\n",
    "\n",
    "X_train, y_train = down_samp_rand(X_train,y_train)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with Max Depth 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error_metric</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.607556</td>\n",
       "      <td>0.553742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.591653</td>\n",
       "      <td>0.266717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.694315</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Error_metric     Train      Test\n",
       "0     Accuracy  0.607556  0.553742\n",
       "1    Precision  0.591653  0.266717\n",
       "2       Recall  0.694315  0.692308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Modelling using AdaBoost with Decision Tree\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "#DT with all features\n",
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=1, random_state=42)\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree with Max Depth 1\")\n",
    "performance_log_data = pd.DataFrame({'Error_metric': ['Accuracy','Precision','Recall'],\n",
    "                               'Train': [accuracy_score(y_train, y_train_pred),\n",
    "                                         precision_score(y_train, y_train_pred, pos_label=1),\n",
    "                                         recall_score(y_train, y_train_pred, pos_label=1)],\n",
    "                               'Test': [accuracy_score(y_test, y_test_pred),\n",
    "                                        precision_score(y_test, y_test_pred, pos_label=1),\n",
    "                                        recall_score(y_test, y_test_pred, pos_label=1)]})\n",
    "\n",
    "display(performance_log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfers_in: 1.0\n",
      "ict_index_lag_avg4: 0.0\n",
      "oppn_team_proj_score: 0.0\n",
      "bonus_lag_avg4: 0.0\n",
      "xP_lag_avg4: 0.0\n",
      "total_points_lag_avg4: 0.0\n",
      "opp_adv_spi: 0.0\n",
      "importance_oppn_team: 0.0\n",
      "importance_p_team: 0.0\n",
      "p_team_proj_score: 0.0\n",
      "minutes_lag_avg4: 0.0\n",
      "oppn_spi: 0.0\n",
      "p_team_spi: 0.0\n",
      "prob_tie: 0.0\n",
      "transfers_out: 0.0\n",
      "selected: 0.0\n",
      "was_home: 0.0\n",
      "bps_lag_avg4: 0.0\n",
      "goals_lag_avg4: 0.0\n",
      "threat_lag_avg4: 0.0\n",
      "shots_lag_avg4: 0.0\n",
      "creativity_lag_avg4: 0.0\n",
      "influence_lag_avg4: 0.0\n",
      "saves_lag_avg4: 0.0\n",
      "penalties_saved_lag_avg4: 0.0\n",
      "penalties_missed_lag_avg4: 0.0\n",
      "goals_conceded_lag_avg4: 0.0\n",
      "clean_sheets_lag_avg4: 0.0\n",
      "yellow_cards_lag_avg4: 0.0\n",
      "xGBuildup_lag_avg4: 0.0\n",
      "xGChain_lag_avg4: 0.0\n",
      "npg_lag_avg4: 0.0\n",
      "key_passes_lag_avg4: 0.0\n",
      "assists_lag_avg4: 0.0\n",
      "xA_lag_avg4: 0.0\n",
      "xG_lag_avg4: 0.0\n",
      "position_id: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Feature importance for Decision Tree\n",
    "\n",
    "importances = tree.feature_importances_\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "for i in sorted_idx:\n",
    "    print(\"{}: {}\".format(X.columns[i], importances[i]))\n",
    "\n",
    "#This will return only 1 important value because it had depth 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error_metric</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.750923</td>\n",
       "      <td>0.640924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.744309</td>\n",
       "      <td>0.315248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.764460</td>\n",
       "      <td>0.664694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Error_metric     Train      Test\n",
       "0     Accuracy  0.750923  0.640924\n",
       "1    Precision  0.744309  0.315248\n",
       "2       Recall  0.764460  0.664694"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=5000, learning_rate=0.5, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "y_train_pred_ada = ada.predict(X_train)\n",
    "y_test_pred_ada = ada.predict(X_test)\n",
    "\n",
    "performance_log_data_ada = pd.DataFrame({'Error_metric': ['Accuracy','Precision','Recall'],\n",
    "                               'Train': [accuracy_score(y_train, y_train_pred_ada),\n",
    "                                         precision_score(y_train, y_train_pred_ada, pos_label=1),\n",
    "                                         recall_score(y_train, y_train_pred_ada, pos_label=1)],\n",
    "                               'Test': [accuracy_score(y_test, y_test_pred_ada),\n",
    "                                        precision_score(y_test, y_test_pred_ada, pos_label=1),\n",
    "                                        recall_score(y_test, y_test_pred_ada, pos_label=1)]})\n",
    "\n",
    "display(performance_log_data_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X_train.columns\n",
    "df = pd.DataFrame(list(zip(feature_names, ada.feature_importances_)))\n",
    "df.columns = ['columns_name', 'score_feature_importance']\n",
    "df.sort_values(by=['score_feature_importance'], ascending = False)\n",
    "column_importance_zero = df[df['score_feature_importance'] < 0.05]['columns_name'].tolist()\n",
    "column_importance_zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
