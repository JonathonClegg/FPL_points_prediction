{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/match_data.csv', delimiter=\";\", index_col=False)\n",
    "player_match_data = pd.read_csv('/Users/jon/Documents/fpl_points_prediction/ERD/tables/player_match_data_withid.csv', delimiter=\";\", index_col=False)\n",
    "\n",
    "#Merge main two tables\n",
    "merged_data = player_match_data.merge(match_data, left_on='match_id', right_on='match_id', how='outer')\n",
    "merged_data = merged_data.drop('value', axis=1)\n",
    "merged_data = merged_data.drop('own_goals', axis=1) #Variation isn't large enough 104\n",
    "merged_data = merged_data.drop('red_cards', axis=1) #Variation isn't large enough 111\n",
    "merged_data = merged_data.drop('npxG', axis=1) #not using npg\n",
    "merged_data = merged_data.drop('player_match_id', axis=1)\n",
    "\n",
    "#Turning 'home' and 'away' into players team and opponent team for analysis\n",
    "merged_data['p_team_spi'] = merged_data.apply(lambda row: row.h_team_spi if row.was_home == 1 else row.a_team_spi, axis=1)\n",
    "merged_data['oppn_spi'] = merged_data.apply(lambda row: row.a_team_spi if row.was_home == 1 else row.h_team_spi, axis=1)\n",
    "merged_data['prob_p_team_win'] = merged_data.apply(lambda row: row.prob_h_win if row.was_home == 1 else row.prob_a_win, axis=1)\n",
    "merged_data['prob_oppn_win'] = merged_data.apply(lambda row: row.prob_a_win if row.was_home == 1 else row.prob_h_win, axis=1)\n",
    "merged_data['p_team_proj_score'] = merged_data.apply(lambda row: row.h_proj_score if row.was_home == 1 else row.a_proj_score, axis=1)\n",
    "merged_data['oppn_team_proj_score'] = merged_data.apply(lambda row: row.a_proj_score if row.was_home == 1 else row.h_proj_score, axis=1)\n",
    "merged_data['importance_p_team'] = merged_data.apply(lambda row: row.importance_h if row.was_home == 1 else row.importance_a, axis=1)\n",
    "merged_data['importance_oppn_team'] = merged_data.apply(lambda row: row.importance_a if row.was_home == 1 else row.importance_h, axis=1)\n",
    "#merged_data['score_p_team'] = merged_data.apply(lambda row: row.h_score if row.was_home == 1 else row.a_score, axis=1)\n",
    "#merged_data['score_oppn_team'] = merged_data.apply(lambda row: row.a_score if row.was_home == 1 else row.h_score, axis=1)\n",
    "#merged_data['xg_p_team'] = merged_data.apply(lambda row: row.h_xg if row.was_home == 1 else row.a_xg, axis=1)\n",
    "#merged_data['xg_oppn_team'] = merged_data.apply(lambda row: row.a_xg if row.was_home == 1 else row.h_xg, axis=1)\n",
    "#Additional features created\n",
    "merged_data['opp_adv_spi'] = merged_data['oppn_spi'] - merged_data['p_team_spi']\n",
    "\n",
    "#Creating lagged varibaled and lagged moving average varibales\n",
    "#Sort values by player_id and date\n",
    "lagged_data = merged_data.sort_values(['player_id', 'date'])\n",
    "\n",
    "#Total points\n",
    "def average_form(var):\n",
    "    lagged_data[var+'-1'] = lagged_data.groupby('player_id')[var].shift(1) #Lagged once\n",
    "    lagged_data[var+'-2'] = lagged_data.groupby('player_id')[var].shift(2) #Lagged twice\n",
    "    lagged_data[var+'-3'] = lagged_data.groupby('player_id')[var].shift(3) #etc.\n",
    "    lagged_data[var+'-4'] = lagged_data.groupby('player_id')[var].shift(4)\n",
    "    lagged_data[var+'_lag_avg2'] = (lagged_data[var+'-1'] + lagged_data[var+'-2'])/2 #lagged 2 week moving average \n",
    "    lagged_data[var+'_lag_avg3'] = (lagged_data[var+'-1'] + lagged_data[var+'-2'] + lagged_data[var+'-3'])/3 #lagged 3 week moving average\n",
    "    lagged_data[var+'_lag_avg4'] = (lagged_data[var+'-1'] + lagged_data[var+'-2'] + lagged_data[var+'-3'] + lagged_data[var+'-4'])/4 #etc.\n",
    "\n",
    "for i, var in enumerate(['total_points','xP','bonus','bps','minutes','goals','shots','xG','xA','assists','key_passes','npg','xGChain','xGBuildup','yellow_cards','clean_sheets','goals_conceded','penalties_missed','penalties_saved','saves','influence','creativity','threat','ict_index']):\n",
    "    average_form(var)\n",
    "\n",
    "#remove date now used for lagging\n",
    "lagged_data = lagged_data.drop('date', axis=1)\n",
    "#Drop rows with NaN values caused by lagging.\n",
    "lagged_data = lagged_data.dropna() \n",
    "\n",
    "#Drop game specific data that is not needed for the model\n",
    "lagged_data = lagged_data.drop(['xP', 'minutes', 'bonus', 'bps', 'goals', 'shots', 'xG', 'xA',\n",
    "       'assists', 'key_passes', 'npg', 'xGChain', 'xGBuildup', 'yellow_cards', 'clean_sheets', 'goals_conceded',\n",
    "       'penalties_missed', 'penalties_saved', 'saves', 'influence',\n",
    "       'creativity', 'threat', 'ict_index', 'round', 'h_team_spi', 'a_team_spi', 'prob_h_win', 'prob_a_win',\n",
    "       'h_proj_score', 'a_proj_score', 'importance_h', 'importance_a',\n",
    "       'h_score', 'a_score', 'h_xg', 'a_xg', 'match_id', 'season_id_y', 'h_nsxg','a_nsxg',], axis=1)\n",
    "\n",
    "#'minutes', better with minutes included. 'season_id_x'\n",
    "#Not dropping match_id as will need it later\n",
    "\n",
    "#Replace total points with return\n",
    "lagged_data['return'] = lagged_data['total_points'].apply(lambda x: 1 if x > 5 else 0)\n",
    "#lagged_data['return'] = lagged_data['total_points'].apply(lambda x: '0' if x <= 2 else '1' if x <= 5 else '2' if x <= 8 else '3')\n",
    "\n",
    "#Drop highly correlated columns and total points\n",
    "lagged_data = lagged_data.drop(['prob_p_team_win', 'prob_oppn_win', 'total_points'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(lagged_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce features\n",
    "lagged_data_fi = lagged_data.drop(column_importance_zero, axis=1)\n",
    "\n",
    "#Limit time or not?\n",
    "#lagged_data_fi = lagged_data_fi[lagged_data_fi['minutes_lag_avg4'] > 60]\n",
    "\n",
    "RAND_STATE = 42 # for reproducible shuffling\n",
    "TT_RATIO = 0.25 # test/train\n",
    "\n",
    "# X,y\n",
    "y = lagged_data['return']\n",
    "X = lagged_data.drop(['return'], axis=1)\n",
    "#X = X.drop(column_importance_zero, axis=1)\n",
    "\n",
    "# test-train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TT_RATIO, random_state=RAND_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3795\n",
       "1    3795\n",
       "Name: return, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#undersampling\n",
    "'''\n",
    "def down_samp_rand(X, y, ratio=1):\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        rus = RandomUnderSampler(sampling_strategy=ratio, random_state=RAND_STATE)\n",
    "        X_rus, y_rus = rus.fit_resample(X, y)\n",
    "        return X_rus, y_rus\n",
    "\n",
    "X_train, y_train = down_samp_rand(X_train,y_train)\n",
    "y_train.value_counts()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15045\n",
       "1    15045\n",
       "Name: return, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upsampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "#fit training data with upsampling\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with Max Depth 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error_metric</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.639482</td>\n",
       "      <td>0.514650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.607192</td>\n",
       "      <td>0.236575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.790096</td>\n",
       "      <td>0.618565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Error_metric     Train      Test\n",
       "0     Accuracy  0.639482  0.514650\n",
       "1    Precision  0.607192  0.236575\n",
       "2       Recall  0.790096  0.618565"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Performance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error_metric</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.883583</td>\n",
       "      <td>0.794427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.973188</td>\n",
       "      <td>0.486068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>0.122465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Error_metric     Train      Test\n",
       "0     Accuracy  0.883583  0.794427\n",
       "1    Precision  0.973188  0.486068\n",
       "2       Recall  0.788900  0.122465"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Modelling using AdaBoost with Decision Tree\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "#DT with all features\n",
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=1, random_state=42)\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree with Max Depth 1\")\n",
    "performance_log_data = pd.DataFrame({'Error_metric': ['Accuracy','Precision','Recall'],\n",
    "                               'Train': [accuracy_score(y_train, y_train_pred),\n",
    "                                         precision_score(y_train, y_train_pred, pos_label=1),\n",
    "                                         recall_score(y_train, y_train_pred, pos_label=1)],\n",
    "                               'Test': [accuracy_score(y_test, y_test_pred),\n",
    "                                        precision_score(y_test, y_test_pred, pos_label=1),\n",
    "                                        recall_score(y_test, y_test_pred, pos_label=1)]})\n",
    "\n",
    "display(performance_log_data)\n",
    "\n",
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=1000, learning_rate=0.5, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "y_train_pred_ada = ada.predict(X_train)\n",
    "y_test_pred_ada = ada.predict(X_test)\n",
    "\n",
    "performance_log_data_ada = pd.DataFrame({'Error_metric': ['Accuracy','Precision','Recall'],\n",
    "                               'Train': [accuracy_score(y_train, y_train_pred_ada),\n",
    "                                         precision_score(y_train, y_train_pred_ada, pos_label=1),\n",
    "                                         recall_score(y_train, y_train_pred_ada, pos_label=1)],\n",
    "                               'Test': [accuracy_score(y_test, y_test_pred_ada),\n",
    "                                        precision_score(y_test, y_test_pred_ada, pos_label=1),\n",
    "                                        recall_score(y_test, y_test_pred_ada, pos_label=1)]})\n",
    "\n",
    "print(\"Ada Boost Performance\")\n",
    "display(performance_log_data_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,4))\n",
    "plot_confusion_matrix(ada,X_train,y_train,ax=ax[0], values_format = 'd')\n",
    "ax[0].title.set_text(\"Train Set\")\n",
    "plot_confusion_matrix(ada,X_test,y_test,ax=ax[1],values_format = 'd')\n",
    "ax[1].title.set_text(\"Test Set\")\n",
    "\n",
    "#Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "folds=3\n",
    "cross_val_scores = cross_val_score(ada, X_train, y_train, cv=folds)\n",
    "print(\"cv scores over {:d} iterations: \\n\".format(folds))\n",
    "cross_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_importance_zero = ['was_home',\n",
    " 'season_id_x',\n",
    " 'total_points-2',\n",
    " 'total_points-3',\n",
    " 'bonus-1',\n",
    " 'bonus-2',\n",
    " 'bonus-3',\n",
    " 'bonus-4',\n",
    " 'bonus_lag_avg2',\n",
    " 'bonus_lag_avg3',\n",
    " 'bonus_lag_avg4',\n",
    " 'minutes-1',\n",
    " 'minutes-2',\n",
    " 'minutes-3',\n",
    " 'minutes-4',\n",
    " 'minutes_lag_avg2',\n",
    " 'goals-1',\n",
    " 'goals-2',\n",
    " 'goals-3',\n",
    " 'goals-4',\n",
    " 'goals_lag_avg2',\n",
    " 'goals_lag_avg3',\n",
    " 'goals_lag_avg4',\n",
    " 'shots-1',\n",
    " 'shots-2',\n",
    " 'shots-3',\n",
    " 'shots-4',\n",
    " 'shots_lag_avg2',\n",
    " 'shots_lag_avg3',\n",
    " 'shots_lag_avg4',\n",
    " 'xG-1',\n",
    " 'xG-2',\n",
    " 'xG-3',\n",
    " 'xA-1',\n",
    " 'xA-2',\n",
    " 'xA-4',\n",
    " 'xA_lag_avg2',\n",
    " 'assists-1',\n",
    " 'assists-2',\n",
    " 'assists-3',\n",
    " 'assists-4',\n",
    " 'assists_lag_avg2',\n",
    " 'assists_lag_avg3',\n",
    " 'assists_lag_avg4',\n",
    " 'key_passes-1',\n",
    " 'key_passes-2',\n",
    " 'key_passes-3',\n",
    " 'key_passes-4',\n",
    " 'key_passes_lag_avg2',\n",
    " 'key_passes_lag_avg3',\n",
    " 'key_passes_lag_avg4',\n",
    " 'npg-1',\n",
    " 'npg-2',\n",
    " 'npg-3',\n",
    " 'npg-4',\n",
    " 'npg_lag_avg2',\n",
    " 'npg_lag_avg3',\n",
    " 'npg_lag_avg4',\n",
    " 'yellow_cards-1',\n",
    " 'yellow_cards-2',\n",
    " 'yellow_cards-3',\n",
    " 'yellow_cards-4',\n",
    " 'yellow_cards_lag_avg2',\n",
    " 'yellow_cards_lag_avg3',\n",
    " 'yellow_cards_lag_avg4',\n",
    " 'clean_sheets-1',\n",
    " 'clean_sheets-2',\n",
    " 'clean_sheets-3',\n",
    " 'clean_sheets-4',\n",
    " 'clean_sheets_lag_avg2',\n",
    " 'clean_sheets_lag_avg3',\n",
    " 'clean_sheets_lag_avg4',\n",
    " 'goals_conceded-1',\n",
    " 'goals_conceded-2',\n",
    " 'goals_conceded-3',\n",
    " 'goals_conceded-4',\n",
    " 'goals_conceded_lag_avg2',\n",
    " 'goals_conceded_lag_avg3',\n",
    " 'goals_conceded_lag_avg4',\n",
    " 'penalties_missed-1',\n",
    " 'penalties_missed-2',\n",
    " 'penalties_missed-3',\n",
    " 'penalties_missed-4',\n",
    " 'penalties_missed_lag_avg2',\n",
    " 'penalties_missed_lag_avg3',\n",
    " 'penalties_missed_lag_avg4',\n",
    " 'penalties_saved-1',\n",
    " 'penalties_saved-2',\n",
    " 'penalties_saved-3',\n",
    " 'penalties_saved-4',\n",
    " 'penalties_saved_lag_avg2',\n",
    " 'penalties_saved_lag_avg3',\n",
    " 'penalties_saved_lag_avg4',\n",
    " 'saves-1',\n",
    " 'saves-2',\n",
    " 'saves-3',\n",
    " 'saves-4',\n",
    " 'saves_lag_avg2',\n",
    " 'saves_lag_avg3',\n",
    " 'saves_lag_avg4',\n",
    " 'threat-1',\n",
    " 'threat-2',\n",
    " 'threat-3']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
